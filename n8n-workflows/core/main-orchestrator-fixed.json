{
  "name": "SADIE Main Orchestrator",
  "active": true,
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "sadie/chat/stream",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook-trigger",
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [250, 300],
      "webhookId": "sadie-main-chat"
    },
    {
      "parameters": {
        "jsCode": "// Extract user message and context\nconst fs = require('fs');\n\nconst body = $input.item.json.body || $input.item.json;\nconst userMessage = body.message || body.text || '';\nconst conversationId = body.conversation_id || 'default';\nconst userId = body.user_id || 'user';\n\n// Load conversation history from memory (Docker-safe path)\nconst memoryDir = '/data/memory';\nconst memoryPath = `${memoryDir}/conversation-history.json`;\nlet conversationHistory = [];\n\ntry {\n  if (!fs.existsSync(memoryDir)) {\n    fs.mkdirSync(memoryDir, { recursive: true });\n  }\n  if (fs.existsSync(memoryPath)) {\n    const data = fs.readFileSync(memoryPath, 'utf8');\n    const allConversations = JSON.parse(data || '{}');\n    conversationHistory = allConversations[conversationId] || [];\n  }\n} catch (error) {\n  console.log('Error loading conversation history:', error.message);\n}\n\n// Sadie system prompt (inline for now)\nconst systemPrompt = `You are Sadie â€” a sweet, supportive, privacy-first AI assistant that runs fully locally.\\n\\nYour purpose:\\n- help the user with tasks through structured tool calls\\n- always output valid JSON when performing actions\\n- remain safe, calm, friendly, and solution-focused\\n\\nRules:\\n1. Be warm, supportive, but professional.\\n2. Never claim human emotions; be gently personable.\\n3. If you cannot perform a task, offer alternatives.\\n4. All tool calls MUST be structured JSON using the tool_call schema.\\n5. Never access blocked paths, perform unsafe operations, or modify system settings.\\n6. Ask for confirmation when required (delete, email sending, risky actions).\\n7. Follow safety policies strictly.\\n8. Keep explanations simple and helpful.`;\n\n// Build Ollama prompt\nconst historyText = conversationHistory\n  .slice(-5)\n  .map(m => `${m.role}: ${m.content}`)\n  .join('\\n');\n\nconst ollamaPrompt = `${systemPrompt}\\n\\nConversation History:\\n${historyText}\\n\\nUser: ${userMessage}\\n\\nRespond in JSON format with:\\n{\\n  \"response\": \"<your reply>\",\\n  \"tool_call\": null OR {\"tool_name\": \"...\", \"parameters\": {...}, \"reasoning\": \"...\"}\\n}`;\n\nreturn {\n  json: {\n    user_message: userMessage,\n    conversation_id: conversationId,\n    user_id: userId,\n    ollama_prompt: ollamaPrompt,\n    history: conversationHistory\n  }\n};"
      },
      "id": "prepare-context",
      "name": "Prepare Context",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [450, 300]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://host.docker.internal:11434/api/generate",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"llama3.2:3b\",\n  \"prompt\": {{ $json.ollama_prompt }},\n  \"stream\": false,\n  \"format\": \"json\"\n}",
        "options": {
          "timeout": 60000
        }
      },
      "id": "call-ollama",
      "name": "Call Ollama",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [650, 300]
    },
    {
      "parameters": {
        "jsCode": "// Parse Ollama response and update memory\nconst fs = require('fs');\n\nconst userMessage = $input.item.json.user_message;\nconst conversationId = $input.item.json.conversation_id;\nconst history = $input.item.json.history || [];\n\n// The HTTP Request node output is merged in; the Ollama body is at $json.response or $json.data.response\nconst rawResponse = $json.response || ($json.data && $json.data.response) || '';\n\nlet ollamaResponse;\ntry {\n  ollamaResponse = JSON.parse(rawResponse);\n} catch (error) {\n  // Fallback if JSON parsing fails\n  ollamaResponse = {\n    response: \"I'm sorry, I had trouble understanding internally. Please try again.\",\n    tool_call: null,\n    error: error.message\n  };\n}\n\n// Save to conversation history (Docker-safe path)\nconst memoryDir = '/data/memory';\nconst memoryPath = `${memoryDir}/conversation-history.json`;\n\nlet allConversations = {};\ntry {\n  if (!fs.existsSync(memoryDir)) {\n    fs.mkdirSync(memoryDir, { recursive: true });\n  }\n  if (fs.existsSync(memoryPath)) {\n    allConversations = JSON.parse(fs.readFileSync(memoryPath, 'utf8') || '{}');\n  }\n} catch (err) {\n  console.log('Error reading conversation history:', err.message);\n}\n\nif (!allConversations[conversationId]) {\n  allConversations[conversationId] = history || [];\n}\n\nallConversations[conversationId].push(\n  { role: 'user', content: userMessage, timestamp: new Date().toISOString() },\n  { role: 'assistant', content: ollamaResponse.response, timestamp: new Date().toISOString() }\n);\n\nif (allConversations[conversationId].length > 100) {\n  allConversations[conversationId] = allConversations[conversationId].slice(-100);\n}\n\ntry {\n  fs.writeFileSync(memoryPath, JSON.stringify(allConversations, null, 2));\n} catch (err) {\n  console.log('Error writing conversation history:', err.message);\n}\n\nreturn {\n  json: {\n    response: ollamaResponse.response,\n    tool_call: ollamaResponse.tool_call || null,\n    error: ollamaResponse.error || null,\n    conversation_id: conversationId,\n    user_message: userMessage\n  }\n};"
      },
      "id": "parse-response",
      "name": "Parse & Save Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [850, 300]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ { response: $json.response, tool_call: $json.tool_call, error: $json.error, conversation_id: $json.conversation_id } }}"
      },
      "id": "respond-direct",
      "name": "Respond Directly",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1050, 300]
    }
  ],
  "connections": {
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "Prepare Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Context": {
      "main": [
        [
          {
            "node": "Call Ollama",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Ollama": {
      "main": [
        [
          {
            "node": "Parse & Save Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse & Save Response": {
      "main": [
        [
          {
            "node": "Respond Directly",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [],
  "triggerCount": 1,
  "updatedAt": "2025-12-08T00:00:00.000Z",
  "versionId": "3"
}
