{
  "name": "SADIE Main Orchestrator",
  "active": true,
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "sadie/chat/stream",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook-trigger",
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1.1,
      "position": [250, 300],
      "webhookId": "sadie-chat"
    },
    {
      "parameters": {
        "jsCode": "// Prepare messages for Ollama /api/chat\nconst body = $input.item.json.body || $input.item.json || {};\nconst userMessage = body.message || body.text || '';\nconst conversationId = body.conversationId || body.conversation_id || 'default';\nconst history = body.history || [];\n\nconst systemPrompt = `You are SADIE (Smart Adaptive Desktop Intelligence Engine), a helpful AI assistant. Be concise, friendly, and helpful. You can help with:\n- Answering questions\n- File operations\n- Web searches\n- Calculations\n- Time and weather queries\n- Memory/recall tasks\n\nAlways be helpful and provide accurate information.`;\n\nconst messages = [\n  { role: 'system', content: systemPrompt }\n];\n\n// Add conversation history if provided\nif (history && history.length > 0) {\n  for (const msg of history) {\n    if (msg.role && msg.content) {\n      messages.push({ role: msg.role, content: msg.content });\n    }\n  }\n}\n\n// Add current user message\nmessages.push({ role: 'user', content: userMessage });\n\nreturn {\n  json: {\n    model: 'llama3.2:3b',\n    messages: messages,\n    stream: false,\n    user_message: userMessage,\n    conversation_id: conversationId\n  }\n};"
      },
      "id": "prepare-request",
      "name": "Prepare Request",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [450, 300]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://host.docker.internal:11434/api/chat",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ {\n  \"model\": $json.model,\n  \"messages\": $json.messages,\n  \"stream\": false\n} }}",
        "options": {
          "response": {
            "response": {
              "neverError": true,
              "responseFormat": "json"
            }
          },
          "timeout": 60000
        }
      },
      "id": "call-ollama",
      "name": "Call Ollama",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [650, 300]
    },
    {
      "parameters": {
        "jsCode": "// Format the response for SADIE widget\nconst ollamaResponse = $input.item.json;\nconst assistantMessage = ollamaResponse.message?.content || ollamaResponse.response || 'Sorry, I could not generate a response.';\n\n// Format as SSE-like chunks for compatibility\nconst chunks = [\n  { type: 'chunk', content: assistantMessage },\n  { type: 'done', content: '' }\n];\n\nreturn {\n  json: {\n    success: true,\n    message: assistantMessage,\n    chunks: chunks,\n    model: ollamaResponse.model || 'llama3.2:3b',\n    done: true\n  }\n};"
      },
      "id": "format-response",
      "name": "Format Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [850, 300]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Content-Type",
                "value": "application/json"
              },
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              }
            ]
          }
        }
      },
      "id": "respond-webhook",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [1050, 300]
    }
  ],
  "connections": {
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "Prepare Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Request": {
      "main": [
        [
          {
            "node": "Call Ollama",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Ollama": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Response": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [],
  "triggerCount": 1,
  "updatedAt": "2026-01-02T00:00:00.000Z",
  "versionId": "1"
}
